\chapter{Introduction}

\section{General Overview of the Problem}

Perception has always been a fundamental task in the field of robotics. In order to perform actions and modify its environment in an automated manner, a robot must be provided with proper knowledge of its surroundings via sensor data, and appropriate processing.

\begin{figure}[ht]
    \includegraphics[width=0.75\textwidth]{perception_action.png}
    \caption{The main ways a robot interacts with the environment: perception and action.}
\end{figure}

One of the most readily available sensors applicable to such a scenario is the color camera. However, effectively utilizing one in a real-time control scenario brings about a series of difficulties and complications. While commonly used sensors, such as thermometers or encoders, usually output a single value, cameras generate data on an entirely different scale. A relatively low resolution 1280x720 pixel RGB sensor outputs an array of 2'774'800 values, thirty to sixty times per second. Furthermore, contrary to what occurs with traditional sensors, these values do not directly reflect a measurment of a physical quantity, but are linked through complex interactions that are difficult to quantify in an analytical manner.

Neural networks, however, are perfectly suited to face these challenges, since they are built to work with a large number of inputs in parallel, and are capable of modelling unknown and complex functions through proper training. In particular, Convolutional Neural Networks (CNNs) have been widely applied to the field of image processing, due to their independance from prior knowledge, the relatively little amount of pre-processing they require, and a decreased subsceptibility to overfitting when compared to their fully-connected counterparts.

\begin{figure}[ht]
    \includegraphics[width=0.6\textwidth]{efficientpose_example.png}
    \caption{Multi-object inference on an image from the LINEMOD\cite{linemod} dataset performed by EfficientPose\cite{bukschat2020efficientpose}, a 6D pose estimation CNN. Green bounding boxes visualize ground truth poses while other colors represent estimations.} 
\end{figure}

Two particular subfields of image processing that have advanced by leaps and bounds in recent years are object identification and pose estimation. Object identification tackles the problem of verifying whether an image contains a particular object, and in which part of the image, while pose estimation techniques provide the position and orientation of the detected object in relation to the camera. Both of these tasks greatly benefit from CNNs, and are very relevant to robotics applications.

However, obtaining accurate and reliable sensor data is only part of the overall perception task in autonomous systems. Another important issue often encountered is the necessity of transforming the raw outputs obatined from real sensors into abstract high-level information. This is ultimately essential for high-level decision making, since is often performed using semantic planners, such as PDDL planners, which rely on symbolic represenations of the state, and are ubiquitous in industrial and collaborative robotics applications.

\section{Thesis Goals}

Our objective in this thesis is therefore to verify the applicability and performance of a state-of-the-art 6D pose estimation CNN to a robotics scenario, and the difficulties involved in doing so. When working with machine learning approaches, one faces two main issues:
\begin{itemize}
    \item The necessity of acquiring large amounts of data to train the network.
    \item The opaqueness of the resulting network to conventional analysis.
\end{itemize}
These two issues are deeply interconnected: neural networks are incapable of understanding cause and effect relationships, being only practiced in percieving and optimizing the relationship between input and output data. Therefore the only way to change their behaviour is to modify their \emph{training inputs}, and associated desired outputs (the \emph{ground truth}), which together form a \emph{dataset}.

However, acquiring the necessary data to build a high-quality dataset is often time-consuming and expenisve. Our initial goal is therefore to improve the applicability of machine learning approaches to pose estimation tasks by facilitating this initial process. For this purpose, we demonstrate an approach for quickly and easily generating large amounts of synthetic labelled data starting from a set of background images, providing realistic depictions of a small number of objects of interest in a determined environment.

\begin{figure}[ht]
    \includegraphics[width=0.9\textwidth]{real_vs_simulated.png}
    \caption{Side-by-side comparison of a real image and a generated training image resulting from our pipeline.}
\end{figure}

We then verify the performance of a model trained on this data, and whether it is capable of effectively generalising to real world conditions.

For many applications in industrial and collaborative robotics, estimations of the pose by themselves may not be sufficient to complete a task, but more high-level knowledge of the semantic state of a scene is required. Therefore we developed a method that uses the outputs from a pose estimation network to compute the overall semantic state of a scene, and tested its reliability in a simple assembly scenario.

Finally, by combining the results of the previous two methods, we verified the performance of the complete system in a real-world application by using the estimations from the trained network and from our semantic meaning extractions algorithm to plan the movement of a robotic manipulator. 

\section{Achieved Results}

Our dataset generation methodology proved to be crucial when testing out different variations of traning inputs and their effect on the final performance of the network. Overall, the resulting system was able to obtain good results and generalise in a satisfactory manner in real-world conditions. The semantics extrapolation method also achieved remarkably good performance, which however is highly dependant on the quality of the initial pose estimations from the network. Finally, applying the combination of pose estimation and semantics extraction methods to a real-life scenario made our robot capable of planning and completing simple assembly tasks, all while relying only on an RGB camera for sensing. However, its reliability may not be sufficient for tasks that require very high precision to be completed, and is again highly dependant on the performance of the underlying neural network.

\section{Thesis structure}
The rest of this thesis is organized as follows.  We will begin by examining the current state-of-the-art for pose estimation approaches (chapter 2), and we will give some background on the pose estimation network we chose as the basis of our approach, EfficientPose (chapter 3). We will then go over our methodologies and underlying motivations (chapter 4), and the metrics and experiments used to evaluate them (chapter 5). Finally, we will lay out the results of our experiments (chapter 6), and conclude the thesis (chapter 7).
